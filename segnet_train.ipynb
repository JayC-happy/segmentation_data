{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "segnet_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMr9PzgkzYzY/1BKL497zJ1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JayC-happy/segmentation_data/blob/master/segnet_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwVDLUAZU3Bh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "e67a609d-66b3-49aa-a021-eee51487f034"
      },
      "source": [
        "! git clone http://github.com/JayC-happy/train.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'train'...\n",
            "warning: redirecting to https://github.com/JayC-happy/train.git/\n",
            "remote: Enumerating objects: 28698, done.\u001b[K\n",
            "remote: Total 28698 (delta 0), reused 0 (delta 0), pack-reused 28698\u001b[K\n",
            "Receiving objects: 100% (28698/28698), 1.57 GiB | 15.23 MiB/s, done.\n",
            "Resolving deltas: 100% (286/286), done.\n",
            "Checking out files: 100% (30000/30000), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukBF_S3PeW58",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8a8cd2c9-c1ab-4c85-8258-4ae70cc3f06e"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data  train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyYjMuWWrrTQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6e0b71e2-c1a2-4874-8a05-ca9f0302d481"
      },
      "source": [
        "#coding=utf-8\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D,MaxPooling2D,UpSampling2D,BatchNormalization,Reshape,Permute,Activation  \n",
        "from keras.utils.np_utils import to_categorical  \n",
        "from keras.preprocessing.image import img_to_array  \n",
        "from keras.callbacks import ModelCheckpoint  \n",
        "from sklearn.preprocessing import LabelEncoder  \n",
        "from PIL import Image\n",
        "import cv2\n",
        "import random\n",
        "import os\n",
        "from tqdm import tqdm \n",
        "\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "seed = 7  \n",
        "np.random.seed(seed)  \n",
        "  \n",
        "#data_shape = 360*480  \n",
        "img_w = 256  \n",
        "img_h = 256  \n",
        "#有一个为背景  \n",
        "n_label = 4 + 1\n",
        "\n",
        "classes = [0., 1., 2., 3., 4.]\n",
        "\n",
        "labelencoder = LabelEncoder()  \n",
        "labelencoder.fit(classes)  \n",
        "\n",
        "image_sets = ['1.png','2.png','3.png']\n",
        "\n",
        "        \n",
        "def load_img(path, grayscale=False):\n",
        "    if grayscale:\n",
        "        img = cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n",
        "    else:\n",
        "        img = cv2.imread(path)\n",
        "        img = np.array(img,dtype=\"float\") / 255.0\n",
        "    return img\n",
        "\n",
        "\n",
        "filepath ='./train/train/'\n",
        "\n",
        "def get_train_val(val_rate = 0.25):\n",
        "    train_url = []    \n",
        "    train_set = []\n",
        "    val_set = []\n",
        "    for pic in os.listdir(filepath + 'src'):\n",
        "        train_url.append(pic)\n",
        "    random.shuffle(train_url)\n",
        "    total_num = len(train_url)\n",
        "    val_num = int(val_rate * total_num)\n",
        "    for i in range(len(train_url)):\n",
        "        if i < val_num:\n",
        "            val_set.append(train_url[i]) \n",
        "        else:\n",
        "            train_set.append(train_url[i])\n",
        "    return train_set, val_set\n",
        "\n",
        "# data for training  \n",
        "def generateData(batch_size,data=[]):  \n",
        "    #print 'generateData...'\n",
        "    while True:  \n",
        "        train_data = []  \n",
        "        train_label = []  \n",
        "        batch = 0  \n",
        "        for i in (range(len(data))): \n",
        "            url = data[i]\n",
        "            batch += 1 \n",
        "            img = load_img(filepath + 'src/' + url)\n",
        "            img = img_to_array(img) \n",
        "            train_data.append(img)  \n",
        "            label = load_img(filepath + 'label/' + url, grayscale=True)\n",
        "            label = img_to_array(label).reshape((img_w * img_h,))  \n",
        "            # print label.shape  \n",
        "            train_label.append(label)  \n",
        "            if batch % batch_size==0: \n",
        "                #print 'get enough bacth!\\n'\n",
        "                train_data = np.array(train_data)  \n",
        "                train_label = np.array(train_label).flatten()  \n",
        "                train_label = labelencoder.transform(train_label)  \n",
        "                train_label = to_categorical(train_label, num_classes=n_label)  \n",
        "                train_label = train_label.reshape((batch_size,img_w * img_h,n_label))  \n",
        "                yield (train_data,train_label)  \n",
        "                train_data = []  \n",
        "                train_label = []  \n",
        "                batch = 0  \n",
        " \n",
        "# data for validation \n",
        "def generateValidData(batch_size,data=[]):  \n",
        "    #print 'generateValidData...'\n",
        "    while True:  \n",
        "        valid_data = []  \n",
        "        valid_label = []  \n",
        "        batch = 0  \n",
        "        for i in (range(len(data))):  \n",
        "            url = data[i]\n",
        "            batch += 1  \n",
        "            img = load_img(filepath + 'src/' + url)\n",
        "            img = img_to_array(img)  \n",
        "            valid_data.append(img)  \n",
        "            label = load_img(filepath + 'label/' + url, grayscale=True)\n",
        "            label = img_to_array(label).reshape((img_w * img_h,))  \n",
        "            # print label.shape  \n",
        "            valid_label.append(label)  \n",
        "            if batch % batch_size==0:  \n",
        "                valid_data = np.array(valid_data)  \n",
        "                valid_label = np.array(valid_label).flatten()  \n",
        "                valid_label = labelencoder.transform(valid_label)  \n",
        "                valid_label = to_categorical(valid_label, num_classes=n_label)  \n",
        "                valid_label = valid_label.reshape((batch_size,img_w * img_h,n_label))  \n",
        "                yield (valid_data,valid_label)  \n",
        "                valid_data = []  \n",
        "                valid_label = []  \n",
        "                batch = 0  \n",
        "  \n",
        "def SegNet():  \n",
        "    model = Sequential()  \n",
        "    #encoder  \n",
        "    model.add(Conv2D(64,(3,3),strides=(1,1),input_shape=(img_w,img_h,3),padding='same',activation='relu'))  \n",
        "    model.add(BatchNormalization())  \n",
        "    model.add(Conv2D(64,(3,3),strides=(1,1),padding='same',activation='relu'))  \n",
        "    model.add(BatchNormalization())  \n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))  \n",
        "    #(128,128)  \n",
        "    model.add(Conv2D(128, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
        "    model.add(BatchNormalization())  \n",
        "    model.add(Conv2D(128, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
        "    model.add(BatchNormalization())  \n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))  \n",
        "    #(64,64)  \n",
        "    model.add(Conv2D(256, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
        "    model.add(BatchNormalization())  \n",
        "    model.add(Conv2D(256, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
        "    model.add(BatchNormalization())  \n",
        "    model.add(Conv2D(256, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
        "    model.add(BatchNormalization())  \n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))  \n",
        "    #(32,32)  \n",
        "    model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
        "    model.add(BatchNormalization())  \n",
        "    model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
        "    model.add(BatchNormalization())  \n",
        "    model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
        "    model.add(BatchNormalization())  \n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))  \n",
        "    #(16,16)  \n",
        "    model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
        "    model.add(BatchNormalization())  \n",
        "    model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
        "    model.add(BatchNormalization())  \n",
        "    model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
        "    model.add(BatchNormalization())  \n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))  \n",
        "    #(8,8)  \n",
        "    #decoder  \n",
        "    model.add(UpSampling2D(size=(2,2)))  \n",
        "    #(16,16)  \n",
        "    model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
        "    model.add(BatchNormalization())  \n",
        "    model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
        "    model.add(BatchNormalization())  \n",
        "    model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
        "    model.add(BatchNormalization())  \n",
        "    model.add(UpSampling2D(size=(2, 2)))  \n",
        "    #(32,32)  \n",
        "    model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
        "    model.add(BatchNormalization())  \n",
        "    model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
        "    model.add(BatchNormalization())  \n",
        "    model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
        "    model.add(BatchNormalization())  \n",
        "    model.add(UpSampling2D(size=(2, 2)))  \n",
        "    #(64,64)  \n",
        "    model.add(Conv2D(256, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
        "    model.add(BatchNormalization())  \n",
        "    model.add(Conv2D(256, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
        "    model.add(BatchNormalization())  \n",
        "    model.add(Conv2D(256, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
        "    model.add(BatchNormalization())  \n",
        "    model.add(UpSampling2D(size=(2, 2)))  \n",
        "    #(128,128)  \n",
        "    model.add(Conv2D(128, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
        "    model.add(BatchNormalization())  \n",
        "    model.add(Conv2D(128, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
        "    model.add(BatchNormalization())  \n",
        "    model.add(UpSampling2D(size=(2, 2)))  \n",
        "    #(256,256)  \n",
        "    model.add(Conv2D(64, (3, 3), strides=(1, 1), input_shape=(3,img_w, img_h), padding='same', activation='relu'))  \n",
        "    model.add(BatchNormalization())  \n",
        "    model.add(Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
        "    model.add(BatchNormalization())  \n",
        "    model.add(Conv2D(n_label, (1, 1), strides=(1, 1), padding='same'))  \n",
        "    model.add(Reshape((n_label,img_w*img_h)))  \n",
        "    #axis=1和axis=2互换位置，等同于np.swapaxes(layer,1,2)  \n",
        "    model.add(Permute((2,1)))  \n",
        "    model.add(Activation('softmax'))  \n",
        "    model.compile(loss='categorical_crossentropy',optimizer='sgd',metrics=['accuracy'])  \n",
        "    model.summary()  \n",
        "    return model  \n",
        "  \n",
        "  \n",
        "def train(): \n",
        "    EPOCHS = 30\n",
        "    BS = 16\n",
        "    model = SegNet()  \n",
        "    modelcheck = ModelCheckpoint('./model',monitor='val_acc',save_best_only=True,mode='max')  \n",
        "    callable = [modelcheck]  \n",
        "    train_set,val_set = get_train_val()\n",
        "    train_numb = len(train_set)  \n",
        "    valid_numb = len(val_set)  \n",
        "    print (\"the number of train data is\",train_numb)  \n",
        "    print (\"the number of val data is\",valid_numb)\n",
        "    H = model.fit_generator(generator=generateData(BS,train_set),steps_per_epoch=train_numb//BS,epochs=EPOCHS,verbose=1,  \n",
        "                    validation_data=generateValidData(BS,val_set),validation_steps=valid_numb//BS,callbacks=callable,max_q_size=1)  \n",
        "\n",
        "    # plot the training loss and accuracy\n",
        "    plt.style.use(\"ggplot\")\n",
        "    plt.figure()\n",
        "    N = EPOCHS\n",
        "    plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
        "    plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "    plt.plot(np.arange(0, N), H.history[\"acc\"], label=\"train_acc\")\n",
        "    plt.plot(np.arange(0, N), H.history[\"val_acc\"], label=\"val_acc\")\n",
        "    plt.title(\"Training Loss and Accuracy on SegNet Satellite Seg\")\n",
        "    plt.xlabel(\"Epoch #\")\n",
        "    plt.ylabel(\"Loss/Accuracy\")\n",
        "    plt.legend(loc=\"lower left\")\n",
        "    plt.savefig(\"plot.png\")\n",
        "\n",
        "\n",
        "#def args_parse():\n",
        "    # construct the argument parse and parse the arguments\n",
        "    #ap = argparse.ArgumentParser()\n",
        "    #ap.add_argument(\"-a\", \"--augment\", help=\"using data augment or not\",\n",
        "    #                action=\"store_true\", default=False)\n",
        "    #ap.add_argument(\"-m\", \"--model\" ,required =True ,\n",
        "    #                help=\"path to output model\")\n",
        "    #ap.add_argument(\"-p\", \"--plot\", type=str, default=\"plot.png\",\n",
        "    #                help=\"path to output accuracy/loss plot\")\n",
        "    #args = vars(ap.parse_args()) \n",
        "    #return args\n",
        "\n",
        "\n",
        "if __name__=='__main__':  \n",
        "    #args = args_parse()\n",
        "    #if args['augment'] == True:\n",
        "        #filepath ='F:/pycode/Satellite-Segmentation-master/train'\n",
        "\n",
        "    train()  \n",
        "    #predict()  \n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 256, 256, 64)      1792      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 256, 256, 64)      256       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 256, 256, 64)      36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 256, 256, 64)      256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 128, 128, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 128, 128, 128)     73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 128, 128, 128)     512       \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 128, 128, 128)     147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 128, 128, 128)     512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 64, 64, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 64, 64, 256)       295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 64, 64, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 64, 64, 256)       590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 64, 64, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 64, 64, 256)       590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 64, 64, 256)       1024      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 32, 32, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 32, 32, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 32, 32, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 32, 32, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 32, 32, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 32, 32, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 32, 32, 512)       2048      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 16, 16, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 16, 16, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 16, 16, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 16, 16, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 16, 16, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 16, 16, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 16, 16, 512)       2048      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 8, 8, 512)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2 (None, 16, 16, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 16, 16, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 16, 16, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 16, 16, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 16, 16, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 16, 16, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 16, 16, 512)       2048      \n",
            "_________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2 (None, 32, 32, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 32, 32, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 32, 32, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 32, 32, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 32, 32, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 32, 32, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 32, 32, 512)       2048      \n",
            "_________________________________________________________________\n",
            "up_sampling2d_3 (UpSampling2 (None, 64, 64, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 64, 64, 256)       1179904   \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 64, 64, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 64, 64, 256)       590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 64, 64, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 64, 64, 256)       590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_22 (Batc (None, 64, 64, 256)       1024      \n",
            "_________________________________________________________________\n",
            "up_sampling2d_4 (UpSampling2 (None, 128, 128, 256)     0         \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 128, 128, 128)     295040    \n",
            "_________________________________________________________________\n",
            "batch_normalization_23 (Batc (None, 128, 128, 128)     512       \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 128, 128, 128)     147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_24 (Batc (None, 128, 128, 128)     512       \n",
            "_________________________________________________________________\n",
            "up_sampling2d_5 (UpSampling2 (None, 256, 256, 128)     0         \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 256, 256, 64)      73792     \n",
            "_________________________________________________________________\n",
            "batch_normalization_25 (Batc (None, 256, 256, 64)      256       \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 256, 256, 64)      36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_26 (Batc (None, 256, 256, 64)      256       \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 256, 256, 5)       325       \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 5, 65536)          0         \n",
            "_________________________________________________________________\n",
            "permute_1 (Permute)          (None, 65536, 5)          0         \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 65536, 5)          0         \n",
            "=================================================================\n",
            "Total params: 31,821,061\n",
            "Trainable params: 31,804,165\n",
            "Non-trainable params: 16,896\n",
            "_________________________________________________________________\n",
            "the number of train data is 11250\n",
            "the number of val data is 3750\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:220: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<generator..., steps_per_epoch=703, epochs=30, verbose=1, validation_data=<generator..., validation_steps=234, callbacks=[<keras.ca..., max_queue_size=1)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "703/703 [==============================] - 548s 779ms/step - loss: 1.3018 - accuracy: 0.4511 - val_loss: 1.1620 - val_accuracy: 0.5478\n",
            "Epoch 2/30\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
            "  'skipping.' % (self.monitor), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "703/703 [==============================] - 531s 755ms/step - loss: 1.0221 - accuracy: 0.5702 - val_loss: 1.1277 - val_accuracy: 0.5753\n",
            "Epoch 3/30\n",
            "703/703 [==============================] - 533s 758ms/step - loss: 0.9169 - accuracy: 0.6175 - val_loss: 0.9343 - val_accuracy: 0.6297\n",
            "Epoch 4/30\n",
            "703/703 [==============================] - 533s 758ms/step - loss: 0.8389 - accuracy: 0.6510 - val_loss: 0.9157 - val_accuracy: 0.6255\n",
            "Epoch 5/30\n",
            "703/703 [==============================] - 533s 758ms/step - loss: 0.7732 - accuracy: 0.6787 - val_loss: 0.8801 - val_accuracy: 0.6569\n",
            "Epoch 6/30\n",
            "703/703 [==============================] - 533s 758ms/step - loss: 0.7115 - accuracy: 0.7044 - val_loss: 0.9288 - val_accuracy: 0.6343\n",
            "Epoch 7/30\n",
            "703/703 [==============================] - 533s 758ms/step - loss: 0.6618 - accuracy: 0.7246 - val_loss: 0.8283 - val_accuracy: 0.6753\n",
            "Epoch 8/30\n",
            "703/703 [==============================] - 533s 759ms/step - loss: 0.6366 - accuracy: 0.7347 - val_loss: 0.8555 - val_accuracy: 0.6804\n",
            "Epoch 9/30\n",
            "703/703 [==============================] - 533s 759ms/step - loss: 0.5981 - accuracy: 0.7506 - val_loss: 0.9047 - val_accuracy: 0.6694\n",
            "Epoch 10/30\n",
            "703/703 [==============================] - 534s 759ms/step - loss: 0.5605 - accuracy: 0.7654 - val_loss: 0.8180 - val_accuracy: 0.6834\n",
            "Epoch 11/30\n",
            "703/703 [==============================] - 533s 758ms/step - loss: 0.5356 - accuracy: 0.7761 - val_loss: 0.8643 - val_accuracy: 0.6873\n",
            "Epoch 12/30\n",
            "703/703 [==============================] - 532s 756ms/step - loss: 0.5077 - accuracy: 0.7885 - val_loss: 0.8476 - val_accuracy: 0.6913\n",
            "Epoch 13/30\n",
            "703/703 [==============================] - 532s 756ms/step - loss: 0.4829 - accuracy: 0.7992 - val_loss: 0.9081 - val_accuracy: 0.6746\n",
            "Epoch 14/30\n",
            "703/703 [==============================] - 532s 757ms/step - loss: 0.4578 - accuracy: 0.8102 - val_loss: 0.9441 - val_accuracy: 0.6843\n",
            "Epoch 15/30\n",
            "703/703 [==============================] - 533s 758ms/step - loss: 0.4426 - accuracy: 0.8176 - val_loss: 0.9359 - val_accuracy: 0.6934\n",
            "Epoch 16/30\n",
            "703/703 [==============================] - 533s 758ms/step - loss: 0.4124 - accuracy: 0.8298 - val_loss: 0.8976 - val_accuracy: 0.6989\n",
            "Epoch 17/30\n",
            "703/703 [==============================] - 532s 757ms/step - loss: 0.3897 - accuracy: 0.8397 - val_loss: 0.9152 - val_accuracy: 0.6984\n",
            "Epoch 18/30\n",
            "703/703 [==============================] - 531s 756ms/step - loss: 0.3711 - accuracy: 0.8474 - val_loss: 0.9310 - val_accuracy: 0.6966\n",
            "Epoch 19/30\n",
            "703/703 [==============================] - 532s 756ms/step - loss: 0.3550 - accuracy: 0.8543 - val_loss: 0.9636 - val_accuracy: 0.6932\n",
            "Epoch 20/30\n",
            "703/703 [==============================] - 532s 756ms/step - loss: 0.3383 - accuracy: 0.8612 - val_loss: 0.9460 - val_accuracy: 0.6986\n",
            "Epoch 21/30\n",
            "703/703 [==============================] - 531s 756ms/step - loss: 0.3241 - accuracy: 0.8672 - val_loss: 1.0041 - val_accuracy: 0.7028\n",
            "Epoch 22/30\n",
            "703/703 [==============================] - 532s 756ms/step - loss: 0.3302 - accuracy: 0.8660 - val_loss: 0.9016 - val_accuracy: 0.7051\n",
            "Epoch 23/30\n",
            "703/703 [==============================] - 532s 757ms/step - loss: 0.3088 - accuracy: 0.8735 - val_loss: 1.0070 - val_accuracy: 0.6986\n",
            "Epoch 24/30\n",
            "703/703 [==============================] - 532s 757ms/step - loss: 0.2896 - accuracy: 0.8814 - val_loss: 1.0628 - val_accuracy: 0.7001\n",
            "Epoch 25/30\n",
            "703/703 [==============================] - 532s 757ms/step - loss: 0.2792 - accuracy: 0.8856 - val_loss: 1.0232 - val_accuracy: 0.7051\n",
            "Epoch 26/30\n",
            "703/703 [==============================] - 532s 757ms/step - loss: 0.2697 - accuracy: 0.8895 - val_loss: 1.0952 - val_accuracy: 0.6942\n",
            "Epoch 27/30\n",
            "703/703 [==============================] - 532s 757ms/step - loss: 0.2604 - accuracy: 0.8933 - val_loss: 1.0824 - val_accuracy: 0.7030\n",
            "Epoch 28/30\n",
            "703/703 [==============================] - 532s 757ms/step - loss: 0.2514 - accuracy: 0.8969 - val_loss: 1.1290 - val_accuracy: 0.7029\n",
            "Epoch 29/30\n",
            " 57/703 [=>............................] - ETA: 7:20 - loss: 0.2439 - accuracy: 0.8998"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sO8dkKATFlf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "965dd20f-0e35-4223-ae50-aaf0dc12465d"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}